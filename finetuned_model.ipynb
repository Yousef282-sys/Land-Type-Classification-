{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "581aa7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class EuroSAT_CNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(EuroSAT_CNN, self).__init__()\n",
    "        \n",
    "        # Block 1\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        # Block 2\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        # Block 3\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.drop1 = nn.Dropout(0.25)\n",
    "        \n",
    "        # Block 4\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        self.drop2 = nn.Dropout(0.25)\n",
    "        \n",
    "        # Block 5\n",
    "        self.conv5 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(512)\n",
    "        self.drop3 = nn.Dropout(0.25)\n",
    "        \n",
    "        # Classifier\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc1 = nn.Linear(512, 256)\n",
    "        self.drop4 = nn.Dropout(0.4)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "        \n",
    "        self.leaky_relu = nn.LeakyReLU(0.1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Block 1\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # Block 2\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # Block 3\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.drop1(x)\n",
    "        \n",
    "        # Block 4\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.drop2(x)\n",
    "        \n",
    "        # Block 5\n",
    "        x = self.conv5(x)\n",
    "        x = self.bn5(x)\n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.drop3(x)\n",
    "        \n",
    "        # Head\n",
    "        x = self.global_pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.drop4(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bbf8cf57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Keras model from final.h5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\moihammed\\miniconda3\\Lib\\site-packages\\keras\\src\\layers\\activations\\leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferring weights...\n",
      "Processing conv1 <- Keras: conv2d_10\n",
      "Processing bn1 <- Keras: batch_normalization_10\n",
      "Processing conv2 <- Keras: conv2d_11\n",
      "Processing bn2 <- Keras: batch_normalization_11\n",
      "Processing conv3 <- Keras: conv2d_12\n",
      "Processing bn3 <- Keras: batch_normalization_12\n",
      "Processing conv4 <- Keras: conv2d_13\n",
      "Processing bn4 <- Keras: batch_normalization_13\n",
      "Processing conv5 <- Keras: conv2d_14\n",
      "Processing bn5 <- Keras: batch_normalization_14\n",
      "Processing fc1 <- Keras: dense_4\n",
      "Processing fc2 <- Keras: dense_5\n",
      "Successfully saved converted model to eurosat_converted.pth\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "def convert_weights(h5_path, pth_output_path):\n",
    "    print(f\"Loading Keras model from {h5_path}...\")\n",
    "    # Load Keras model\n",
    "    keras_model = load_model(h5_path, compile=False)\n",
    "    \n",
    "    # Instantiate PyTorch model\n",
    "    # Note: Keras model had 10 classes (EuroSAT default)\n",
    "    torch_model = EuroSAT_CNN(num_classes=10) \n",
    "    torch_model.eval()\n",
    "    \n",
    "    # Helper to get layer weights from Keras model\n",
    "    # We iterate manually to match the specific architecture\n",
    "    k_layers = [l for l in keras_model.layers if len(l.get_weights()) > 0]\n",
    "    \n",
    "    # Mapping logic:\n",
    "    # Keras Conv2D weights: [H, W, In, Out] -> PyTorch: [Out, In, H, W]\n",
    "    # Keras Dense weights: [In, Out] -> PyTorch: [Out, In]\n",
    "    \n",
    "    # We define the sequence of PyTorch layers that have weights\n",
    "    # based on the order they appear in the forward pass / __init__\n",
    "    torch_layer_names = [\n",
    "        'conv1', 'bn1', \n",
    "        'conv2', 'bn2', \n",
    "        'conv3', 'bn3', \n",
    "        'conv4', 'bn4', \n",
    "        'conv5', 'bn5', \n",
    "        'fc1', 'fc2'\n",
    "    ]\n",
    "    \n",
    "    k_idx = 0\n",
    "    state_dict = torch_model.state_dict()\n",
    "    \n",
    "    print(\"Transferring weights...\")\n",
    "    \n",
    "    for name in torch_layer_names:\n",
    "        layer = getattr(torch_model, name)\n",
    "        \n",
    "        # Skip Keras layers that don't have weights (Dropout, Pooling, etc are skipped in k_layers list)\n",
    "        while k_idx < len(k_layers) and len(k_layers[k_idx].get_weights()) == 0:\n",
    "            k_idx += 1\n",
    "            \n",
    "        if k_idx >= len(k_layers):\n",
    "            break\n",
    "            \n",
    "        k_layer = k_layers[k_idx]\n",
    "        k_weights = k_layer.get_weights()\n",
    "        \n",
    "        print(f\"Processing {name} <- Keras: {k_layer.name}\")\n",
    "        \n",
    "        if isinstance(layer, torch.nn.Conv2d):\n",
    "            # Keras: [H, W, In, Out], Bias [Out]\n",
    "            # PyTorch: [Out, In, H, W], Bias [Out]\n",
    "            weight = k_weights[0]\n",
    "            bias = k_weights[1] if len(k_weights) > 1 else None\n",
    "            \n",
    "            # Transpose: (H, W, In, Out) -> (Out, In, H, W)\n",
    "            weight = np.transpose(weight, (3, 2, 0, 1))\n",
    "            \n",
    "            state_dict[f'{name}.weight'] = torch.from_numpy(weight)\n",
    "            if bias is not None:\n",
    "                state_dict[f'{name}.bias'] = torch.from_numpy(bias)\n",
    "                \n",
    "        elif isinstance(layer, torch.nn.BatchNorm2d):\n",
    "            # Keras BN: [gamma, beta, mean, variance]\n",
    "            # PyTorch BN: weight(gamma), bias(beta), running_mean, running_var\n",
    "            gamma, beta, mean, var = k_weights\n",
    "            \n",
    "            state_dict[f'{name}.weight'] = torch.from_numpy(gamma)\n",
    "            state_dict[f'{name}.bias'] = torch.from_numpy(beta)\n",
    "            state_dict[f'{name}.running_mean'] = torch.from_numpy(mean)\n",
    "            state_dict[f'{name}.running_var'] = torch.from_numpy(var)\n",
    "            \n",
    "        elif isinstance(layer, torch.nn.Linear):\n",
    "            # Keras: [In, Out]\n",
    "            # PyTorch: [Out, In]\n",
    "            weight = k_weights[0]\n",
    "            bias = k_weights[1] if len(k_weights) > 1 else None\n",
    "            \n",
    "            weight = np.transpose(weight, (1, 0))\n",
    "            \n",
    "            state_dict[f'{name}.weight'] = torch.from_numpy(weight)\n",
    "            if bias is not None:\n",
    "                state_dict[f'{name}.bias'] = torch.from_numpy(bias)\n",
    "        \n",
    "        k_idx += 1\n",
    "\n",
    "    # Save\n",
    "    torch_model.load_state_dict(state_dict)\n",
    "    torch.save(torch_model.state_dict(), pth_output_path)\n",
    "    print(f\"Successfully saved converted model to {pth_output_path}\")\n",
    "\n",
    "# Run the conversion\n",
    "h5_file = \"final.h5\" \n",
    "convert_weights(h5_file, \"eurosat_converted.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82454da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Detected 3 classes: ['Crop Data', 'Desert Data', 'Urban Data']\n",
      "Converted weights loaded successfully.\n",
      "Epoch [1/10] Loss: 0.7472 | Train Acc: 69.36%\n",
      "Epoch [1/10] Loss: 0.7472 | Train Acc: 69.36%\n",
      "Validation Acc: 88.57%\n",
      "Validation Acc: 88.57%\n",
      "Epoch [2/10] Loss: 0.3680 | Train Acc: 90.66%\n",
      "Epoch [2/10] Loss: 0.3680 | Train Acc: 90.66%\n",
      "Validation Acc: 94.64%\n",
      "Validation Acc: 94.64%\n",
      "Epoch [3/10] Loss: 0.2056 | Train Acc: 95.06%\n",
      "Epoch [3/10] Loss: 0.2056 | Train Acc: 95.06%\n",
      "Validation Acc: 96.07%\n",
      "Validation Acc: 96.07%\n",
      "Epoch [4/10] Loss: 0.1383 | Train Acc: 96.32%\n",
      "Epoch [4/10] Loss: 0.1383 | Train Acc: 96.32%\n",
      "Validation Acc: 96.07%\n",
      "Validation Acc: 96.07%\n",
      "Epoch [5/10] Loss: 0.1038 | Train Acc: 97.21%\n",
      "Epoch [5/10] Loss: 0.1038 | Train Acc: 97.21%\n",
      "Validation Acc: 97.50%\n",
      "Validation Acc: 97.50%\n",
      "Epoch [6/10] Loss: 0.0917 | Train Acc: 97.04%\n",
      "Epoch [6/10] Loss: 0.0917 | Train Acc: 97.04%\n",
      "Validation Acc: 97.50%\n",
      "Validation Acc: 97.50%\n",
      "Epoch [7/10] Loss: 0.0930 | Train Acc: 97.04%\n",
      "Epoch [7/10] Loss: 0.0930 | Train Acc: 97.04%\n",
      "Validation Acc: 98.21%\n",
      "Epoch [8/10] Loss: 0.0686 | Train Acc: 97.93%\n",
      "Validation Acc: 98.21%\n",
      "Epoch [9/10] Loss: 0.0668 | Train Acc: 98.20%\n",
      "Validation Acc: 97.86%\n",
      "Epoch [10/10] Loss: 0.0589 | Train Acc: 98.29%\n",
      "Validation Acc: 97.50%\n",
      "Fine-tuning complete. Model saved.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "def fine_tune():\n",
    "    # --- Configuration ---\n",
    "    DATA_DIR = 'Egypt_Data_Split' # Assumes folders 'train' and 'test' inside\n",
    "    CONVERTED_MODEL_PATH = 'eurosat_converted.pth'\n",
    "    BATCH_SIZE = 32\n",
    "    LEARNING_RATE = 0.0001\n",
    "    EPOCHS = 10\n",
    "    IMG_SIZE = 128\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "    # --- Data Transforms ---\n",
    "    # Matches the preprocessing in the notebook (rescale 1/255 is handled by ToTensor)\n",
    "    train_transforms = transforms.Compose([\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(20),\n",
    "        transforms.ToTensor(), # Converts 0-255 to 0.0-1.0\n",
    "    ])\n",
    "\n",
    "    val_transforms = transforms.Compose([\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    # --- Load Data ---\n",
    "    train_dir = os.path.join(DATA_DIR, 'train')\n",
    "    test_dir = os.path.join(DATA_DIR, 'test')\n",
    "\n",
    "    if not os.path.exists(train_dir):\n",
    "        print(f\"Error: Directory {train_dir} not found.\")\n",
    "        return\n",
    "\n",
    "    train_dataset = datasets.ImageFolder(train_dir, transform=train_transforms)\n",
    "    val_dataset = datasets.ImageFolder(test_dir, transform=val_transforms)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    num_new_classes = len(train_dataset.classes)\n",
    "    print(f\"Detected {num_new_classes} new Egypt classes: {train_dataset.classes}\")\n",
    "\n",
    "    # --- Load Model ---\n",
    "    # 1. Initialize with original 10 classes to load weights correctly\n",
    "    model = EuroSAT_CNN(num_classes=10)\n",
    "    \n",
    "    # 2. Load the converted weights\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(CONVERTED_MODEL_PATH))\n",
    "        print(\"Converted weights loaded successfully.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Please run the conversion script first!\")\n",
    "        return\n",
    "\n",
    "    # 3. EXPAND the final layer to support 10 original + 3 new classes = 13 total\n",
    "    total_classes = 10 + num_new_classes  # 10 EuroSAT + 3 Egypt = 13 classes\n",
    "    in_features = model.fc2.in_features\n",
    "    \n",
    "    # Save old weights\n",
    "    old_fc2_weight = model.fc2.weight.data.clone()\n",
    "    old_fc2_bias = model.fc2.bias.data.clone()\n",
    "    \n",
    "    # Create new larger final layer\n",
    "    model.fc2 = nn.Linear(in_features, total_classes)\n",
    "    \n",
    "    # Copy old weights to the new layer (for first 10 classes)\n",
    "    model.fc2.weight.data[:10] = old_fc2_weight\n",
    "    model.fc2.bias.data[:10] = old_fc2_bias\n",
    "    \n",
    "    # Initialize new class weights randomly (for classes 10-12)\n",
    "    nn.init.xavier_uniform_(model.fc2.weight.data[10:])\n",
    "    nn.init.zeros_(model.fc2.bias.data[10:])\n",
    "    \n",
    "    print(f\"Model expanded to predict {total_classes} classes:\")\n",
    "    print(f\"  - Classes 0-9: Original EuroSAT classes (weights preserved)\")\n",
    "    print(f\"  - Classes 10-{total_classes-1}: New Egypt classes (Crop Data, Desert Data, Urban Data)\")\n",
    "    \n",
    "    model = model.to(DEVICE)\n",
    "\n",
    "    # --- Setup Training ---\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    # --- Adjust labels: Egypt classes should be mapped to indices 10, 11, 12 ---\n",
    "    # The DataLoader gives labels 0, 1, 2 for Egypt classes\n",
    "    # We need to shift them to 10, 11, 12\n",
    "    \n",
    "    # --- Training Loop ---\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            # Shift labels from [0,1,2] to [10,11,12]\n",
    "            labels = labels + 10\n",
    "            labels = labels.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        epoch_acc = 100 * correct / total\n",
    "        print(f\"Epoch [{epoch+1}/{EPOCHS}] Loss: {running_loss/len(train_loader):.4f} | Train Acc: {epoch_acc:.2f}%\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs = inputs.to(DEVICE)\n",
    "                # Shift labels from [0,1,2] to [10,11,12]\n",
    "                labels = labels + 10\n",
    "                labels = labels.to(DEVICE)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_acc = 100 * val_correct / val_total\n",
    "        print(f\"Validation Acc: {val_acc:.2f}%\")\n",
    "\n",
    "    # Save Fine-tuned model\n",
    "    torch.save(model.state_dict(), 'finetuned_egypt_model.pth')\n",
    "    print(f\"\\nFine-tuning complete. Model saved with {total_classes} classes.\")\n",
    "    print(\"Class mapping:\")\n",
    "    print(\"  0-9: Original EuroSAT classes\")\n",
    "    print(\"  10: Crop Data\")\n",
    "    print(\"  11: Desert Data\")\n",
    "    print(\"  12: Urban Data\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    fine_tune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4501067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MODEL ARCHITECTURE\n",
      "============================================================\n",
      "EuroSAT_CNN(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (drop1): Dropout(p=0.25, inplace=False)\n",
      "  (conv4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (drop2): Dropout(p=0.25, inplace=False)\n",
      "  (conv5): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (drop3): Dropout(p=0.25, inplace=False)\n",
      "  (global_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc1): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (drop4): Dropout(p=0.4, inplace=False)\n",
      "  (fc2): Linear(in_features=256, out_features=3, bias=True)\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n",
      "\n",
      "\n",
      "============================================================\n",
      "CLASSES THE MODEL PREDICTS\n",
      "============================================================\n",
      "Class 0: Crop Data\n",
      "Class 1: Desert Data\n",
      "Class 2: Urban Data\n",
      "\n",
      "\n",
      "============================================================\n",
      "MODEL SUMMARY\n",
      "============================================================\n",
      "Install torchsummary for detailed model summary: pip install torchsummary\n",
      "\n",
      "Total parameters: 1,702,659\n",
      "Trainable parameters: 1,702,659\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchsummary import torchsummary\n",
    "from torchvision import datasets\n",
    "\n",
    "# Load the fine-tuned model with 13 classes\n",
    "model = EuroSAT_CNN(num_classes=13)  # 10 EuroSAT + 3 Egypt = 13 classes\n",
    "model.load_state_dict(torch.load('finetuned_egypt_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL ARCHITECTURE\")\n",
    "print(\"=\"*60)\n",
    "print(model)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Define all class names\n",
    "eurosat_classes = [\n",
    "    \"AnnualCrop\", \"Forest\", \"HerbaceousVegetation\", \"Highway\", \"Industrial\",\n",
    "    \"Pasture\", \"PermanentCrop\", \"Residential\", \"River\", \"SeaLake\"\n",
    "]\n",
    "\n",
    "egypt_classes = [\"Crop Data\", \"Desert Data\", \"Urban Data\"]\n",
    "\n",
    "all_classes = eurosat_classes + egypt_classes\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ALL CLASSES THE MODEL CAN PREDICT\")\n",
    "print(\"=\"*60)\n",
    "for idx, class_name in enumerate(all_classes):\n",
    "    if idx < 10:\n",
    "        print(f\"Class {idx}: {class_name} (EuroSAT - original)\")\n",
    "    else:\n",
    "        print(f\"Class {idx}: {class_name} (Egypt - new)\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "try:\n",
    "    from torchsummary import summary\n",
    "    summary(model, (3, 128, 128))\n",
    "except:\n",
    "    print(\"Install torchsummary for detailed model summary: pip install torchsummary\")\n",
    "    print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0147baa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Load the fine-tuned model\n",
    "model = EuroSAT_CNN(num_classes=13)\n",
    "model.load_state_dict(torch.load('finetuned_egypt_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Get number of classes from the model\n",
    "num_classes = model.fc2.out_features\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL INFORMATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total number of classes: {num_classes}\")\n",
    "print(f\"  - Original EuroSAT classes: 10 (indices 0-9)\")\n",
    "print(f\"  - New Egypt classes: 3 (indices 10-12)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Define all class names\n",
    "all_classes = [\n",
    "    \"0: AnnualCrop (EuroSAT)\",\n",
    "    \"1: Forest (EuroSAT)\",\n",
    "    \"2: HerbaceousVegetation (EuroSAT)\",\n",
    "    \"3: Highway (EuroSAT)\",\n",
    "    \"4: Industrial (EuroSAT)\",\n",
    "    \"5: Pasture (EuroSAT)\",\n",
    "    \"6: PermanentCrop (EuroSAT)\",\n",
    "    \"7: Residential (EuroSAT)\",\n",
    "    \"8: River (EuroSAT)\",\n",
    "    \"9: SeaLake (EuroSAT)\",\n",
    "    \"10: Crop Data (Egypt)\",\n",
    "    \"11: Desert Data (Egypt)\",\n",
    "    \"12: Urban Data (Egypt)\"\n",
    "]\n",
    "\n",
    "print(\"\\nALL CLASSES:\")\n",
    "for class_name in all_classes:\n",
    "    print(f\"  {class_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
