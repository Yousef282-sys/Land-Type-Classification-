{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "580758e3-c202-4526-9ea3-d945b9a1865f",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "580758e3-c202-4526-9ea3-d945b9a1865f"
      },
      "source": [
        "## Step 2: Data Exploration:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "mNmjt_LRVULZ",
        "outputId": "2236798c-2671-4b1b-e0da-35c97b614b3c"
      },
      "id": "mNmjt_LRVULZ",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2d1fe916-6257-49f2-b27b-9d39e4e7186d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-2d1fe916-6257-49f2-b27b-9d39e4e7186d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Egypt_Data_Split.zip to Egypt_Data_Split.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_path = \"Egypt_Data_Split.zip\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(\".\")\n",
        "\n",
        "print(\"Extracted Successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fsbWHNLVm76",
        "outputId": "e6bc45b5-99db-4084-c82d-27be0d931593"
      },
      "id": "9fsbWHNLVm76",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted Successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "1d3e97cf-667a-46b4-9e4f-3ac16a33addf",
      "metadata": {
        "id": "1d3e97cf-667a-46b4-9e4f-3ac16a33addf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import seaborn as sns\n",
        "import cv2\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "d7540ada-940d-4523-8d19-1d5759c8517c",
      "metadata": {
        "id": "d7540ada-940d-4523-8d19-1d5759c8517c"
      },
      "outputs": [],
      "source": [
        "dataset_path = \"Egypt_Data_Split/train\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "caac1d87-cdcf-4411-8486-cd634ad2eb0a",
      "metadata": {
        "id": "caac1d87-cdcf-4411-8486-cd634ad2eb0a"
      },
      "outputs": [],
      "source": [
        "# List all land type categories\n",
        "categories = os.listdir(dataset_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20f12034-ca92-4f12-88d6-1c228df455cd",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "20f12034-ca92-4f12-88d6-1c228df455cd"
      },
      "source": [
        "## Step 3: Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "92d05cab-c30b-4d48-9089-5b1738e4ee7e",
      "metadata": {
        "id": "92d05cab-c30b-4d48-9089-5b1738e4ee7e"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "a3edd8b8-017b-4d12-a3ea-81bc12a1feeb",
      "metadata": {
        "id": "a3edd8b8-017b-4d12-a3ea-81bc12a1feeb"
      },
      "outputs": [],
      "source": [
        "# Resize, normalize, and convert to RGB\n",
        "def load_and_preprocess_images(path, size=(64, 64)):\n",
        "    data, labels = [], []\n",
        "    for category in categories:\n",
        "        category_path = os.path.join(path, category)\n",
        "        for img_name in os.listdir(category_path):\n",
        "            img = cv2.imread(os.path.join(category_path, img_name))\n",
        "            img = cv2.resize(img, size)\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
        "            img = img / 255.0  # Normalize to [0,1]\n",
        "            data.append(img)\n",
        "            labels.append(category)\n",
        "    return np.array(data, dtype=\"float32\"), np.array(labels)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_preprocess_images(path, size=(128,128)):\n",
        "    data = []\n",
        "    labels = []\n",
        "\n",
        "    for category in os.listdir(path):\n",
        "        category_path = os.path.join(path, category)\n",
        "\n",
        "        # Skip non-folder items\n",
        "        if not os.path.isdir(category_path):\n",
        "            continue\n",
        "\n",
        "        for img_name in os.listdir(category_path):\n",
        "            img_path = os.path.join(category_path, img_name)\n",
        "\n",
        "            # Attempt to read image\n",
        "            img = cv2.imread(img_path)\n",
        "\n",
        "            # If image can't be read → skip it\n",
        "            if img is None:\n",
        "                print(\"Skipped unreadable file:\", img_path)\n",
        "                continue\n",
        "\n",
        "            img = cv2.resize(img, size)\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            img = img / 255.0\n",
        "\n",
        "            data.append(img)\n",
        "            labels.append(category)\n",
        "\n",
        "    return np.array(data), np.array(labels)\n"
      ],
      "metadata": {
        "id": "m3p2GhpZWovu"
      },
      "id": "m3p2GhpZWovu",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_preprocess_images(path, size=(128,128)):\n",
        "    data = []\n",
        "    labels = []\n",
        "\n",
        "    # Loop over each class folder\n",
        "    for category in os.listdir(path):\n",
        "        category_path = os.path.join(path, category)\n",
        "\n",
        "        # تأكد إنه فولدر مش فايل\n",
        "        if not os.path.isdir(category_path):\n",
        "            continue\n",
        "\n",
        "        # Loop over images inside class folder\n",
        "        for img_name in os.listdir(category_path):\n",
        "            img_path = os.path.join(category_path, img_name)\n",
        "\n",
        "            # Read image\n",
        "            img = cv2.imread(img_path)\n",
        "\n",
        "            # Skip unreadable or non-image files\n",
        "            if img is None:\n",
        "                # print(\"Skipped unreadable file:\", img_path)\n",
        "                continue\n",
        "\n",
        "            img = cv2.resize(img, size)\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            img = img / 255.0\n",
        "\n",
        "            data.append(img)\n",
        "            labels.append(category)\n",
        "\n",
        "    return np.array(data), np.array(labels)\n"
      ],
      "metadata": {
        "id": "ONpJOzR4XQHF"
      },
      "id": "ONpJOzR4XQHF",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "8f8c6529-1af7-4793-a180-19055e22a8f8",
      "metadata": {
        "id": "8f8c6529-1af7-4793-a180-19055e22a8f8"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "data, labels = load_and_preprocess_images(dataset_path, size=(128, 128))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "96f8fc09-adac-46c1-9c78-0dbe8f986924",
      "metadata": {
        "id": "96f8fc09-adac-46c1-9c78-0dbe8f986924"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "cfde6993-87d5-4637-bf88-aed0769f611e",
      "metadata": {
        "id": "cfde6993-87d5-4637-bf88-aed0769f611e"
      },
      "outputs": [],
      "source": [
        "# Encode labels into integers\n",
        "label_encoder = LabelEncoder()\n",
        "labels_encoded = label_encoder.fit_transform(labels)  # Convert category names to integer labels"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(labels)\n",
        "set(labels)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXj3AeH_YfO-",
        "outputId": "c528aa4c-a8aa-4dc7-deb1-4a2683c887e5"
      },
      "id": "iXj3AeH_YfO-",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{np.str_('Crop Data'), np.str_('Desert Data'), np.str_('Urban Data')}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "labels_encoded = label_encoder.fit_transform(labels)\n",
        "\n",
        "print(\"Encoded labels example:\", labels_encoded[:10])\n",
        "print(\"Classes:\", label_encoder.classes_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLOjIOeNZHAg",
        "outputId": "ab5008c7-8314-4905-a406-c5ca13a05ba1"
      },
      "id": "VLOjIOeNZHAg",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded labels example: [1 1 1 1 1 1 1 1 1 1]\n",
            "Classes: ['Crop Data' 'Desert Data' 'Urban Data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "labels_categorical = to_categorical(labels_encoded, num_classes=len(label_encoder.classes_))\n",
        "\n",
        "print(\"One-hot shape:\", labels_categorical.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7_fBRDCZIrL",
        "outputId": "a7605f2c-d971-43be-de40-6b0621c9fa5f"
      },
      "id": "i7_fBRDCZIrL",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One-hot shape: (1113, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "24e3dce9-cbf9-4de4-9bec-20126297e3b1",
      "metadata": {
        "id": "24e3dce9-cbf9-4de4-9bec-20126297e3b1"
      },
      "outputs": [],
      "source": [
        "# Convert to one-hot encoding\n",
        "labels_categorical = to_categorical(labels_encoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "8ede2900-1f48-4573-a163-6879be5073a6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ede2900-1f48-4573-a163-6879be5073a6",
        "outputId": "8783061c-625f-4e7a-ab19-4cebb6ed4179"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data: (1113, 128, 128, 3)\n",
            "Labels: (1113, 3)\n",
            "Train samples: 779\n",
            "Validation samples: 222\n",
            "Test samples: 112\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# تأكد إن X و y لهم نفس العدد\n",
        "print(\"Data:\", data.shape)\n",
        "print(\"Labels:\", labels_categorical.shape)\n",
        "\n",
        "# Shuffle قبل التقسيم\n",
        "indices = np.arange(len(data))\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "data_shuffled = data[indices]\n",
        "labels_shuffled = labels_categorical[indices]\n",
        "\n",
        "# تقسيم 70% Train و 30% Temp\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    data_shuffled, labels_shuffled,\n",
        "    test_size=0.3,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# تقسيم الـ Temp إلى 20% Val و 10% Test\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp,\n",
        "    test_size=1/3,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# طباعة أعداد العينات للتأكد\n",
        "print(\"Train samples:\", len(X_train))\n",
        "print(\"Validation samples:\", len(X_val))\n",
        "print(\"Test samples:\", len(X_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "model = models.Sequential([\n",
        "\n",
        "    # اول طبقة Convolution\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    # الطبقة التانية\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    # الطبقة التالتة\n",
        "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    # Flatten\n",
        "    layers.Flatten(),\n",
        "\n",
        "    # Fully connected\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.3),\n",
        "\n",
        "    # Output layer (3 classes)\n",
        "    layers.Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        },
        "id": "9BHs-EUJbehe",
        "outputId": "6d3e2727-f831-4234-8f84-6d9287a6df58"
      },
      "id": "9BHs-EUJbehe",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25088\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │     \u001b[38;5;34m3,211,392\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │           \u001b[38;5;34m387\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25088</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,211,392</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">387</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,305,027\u001b[0m (12.61 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,305,027</span> (12.61 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,305,027\u001b[0m (12.61 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,305,027</span> (12.61 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=20,\n",
        "    batch_size=32\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gO-mZDyKbjBA",
        "outputId": "256b0d68-39e4-4154-cc8b-989d334c3d62"
      },
      "id": "gO-mZDyKbjBA",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 1s/step - accuracy: 0.5258 - loss: 0.9046 - val_accuracy: 0.7072 - val_loss: 0.6219\n",
            "Epoch 2/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 1s/step - accuracy: 0.7898 - loss: 0.4913 - val_accuracy: 0.9324 - val_loss: 0.1835\n",
            "Epoch 3/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 1s/step - accuracy: 0.9083 - loss: 0.2893 - val_accuracy: 0.9189 - val_loss: 0.2854\n",
            "Epoch 4/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step - accuracy: 0.9295 - loss: 0.1756 - val_accuracy: 0.9234 - val_loss: 0.1746\n",
            "Epoch 5/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step - accuracy: 0.9621 - loss: 0.1227 - val_accuracy: 0.9324 - val_loss: 0.1789\n",
            "Epoch 6/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step - accuracy: 0.9607 - loss: 0.1318 - val_accuracy: 0.9324 - val_loss: 0.1953\n",
            "Epoch 7/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 1s/step - accuracy: 0.9516 - loss: 0.1162 - val_accuracy: 0.7973 - val_loss: 0.8757\n",
            "Epoch 8/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 1s/step - accuracy: 0.8643 - loss: 0.4216 - val_accuracy: 0.9369 - val_loss: 0.1933\n",
            "Epoch 9/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 1s/step - accuracy: 0.9535 - loss: 0.1400 - val_accuracy: 0.9369 - val_loss: 0.1827\n",
            "Epoch 10/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 1s/step - accuracy: 0.9525 - loss: 0.1129 - val_accuracy: 0.9189 - val_loss: 0.2530\n",
            "Epoch 11/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 1s/step - accuracy: 0.9488 - loss: 0.1319 - val_accuracy: 0.9369 - val_loss: 0.1867\n",
            "Epoch 12/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 1s/step - accuracy: 0.9826 - loss: 0.0652 - val_accuracy: 0.9234 - val_loss: 0.2790\n",
            "Epoch 13/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step - accuracy: 0.9845 - loss: 0.0654 - val_accuracy: 0.9550 - val_loss: 0.1868\n",
            "Epoch 14/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 1s/step - accuracy: 0.9848 - loss: 0.0663 - val_accuracy: 0.9414 - val_loss: 0.2567\n",
            "Epoch 15/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 1s/step - accuracy: 0.9783 - loss: 0.0622 - val_accuracy: 0.9414 - val_loss: 0.2530\n",
            "Epoch 16/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 990ms/step - accuracy: 0.9764 - loss: 0.0645 - val_accuracy: 0.9414 - val_loss: 0.2516\n",
            "Epoch 17/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 1s/step - accuracy: 0.9901 - loss: 0.0356 - val_accuracy: 0.9505 - val_loss: 0.1814\n",
            "Epoch 18/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 1s/step - accuracy: 0.9884 - loss: 0.0471 - val_accuracy: 0.9459 - val_loss: 0.1757\n",
            "Epoch 19/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step - accuracy: 0.9890 - loss: 0.0304 - val_accuracy: 0.8829 - val_loss: 0.6933\n",
            "Epoch 20/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 1s/step - accuracy: 0.9657 - loss: 0.0894 - val_accuracy: 0.9414 - val_loss: 0.3006\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=3,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "checkpoint = ModelCheckpoint(\n",
        "    'best_model.h5',\n",
        "    monitor='val_accuracy',\n",
        "    save_best_only=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "Hfyme4Crfels"
      },
      "id": "Hfyme4Crfels",
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=30,\n",
        "    batch_size=32,\n",
        "    callbacks=[early_stop, checkpoint]\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Clct3KTrfgBS",
        "outputId": "e037ae17-1ea9-4fec-b0e2-db4895924d45"
      },
      "id": "Clct3KTrfgBS",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 992ms/step - accuracy: 0.9954 - loss: 0.0145 - val_accuracy: 0.9505 - val_loss: 0.2912\n",
            "Epoch 2/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 1s/step - accuracy: 0.9891 - loss: 0.0315 - val_accuracy: 0.9279 - val_loss: 0.4095\n",
            "Epoch 3/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step - accuracy: 0.9970 - loss: 0.0175 - val_accuracy: 0.9279 - val_loss: 0.4584\n",
            "Epoch 4/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 959ms/step - accuracy: 0.9799 - loss: 0.0603"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step - accuracy: 0.9803 - loss: 0.0596 - val_accuracy: 0.9595 - val_loss: 0.3448\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "448a0fab-6336-4e58-af34-cca5e19c538e",
      "metadata": {
        "id": "448a0fab-6336-4e58-af34-cca5e19c538e"
      },
      "outputs": [],
      "source": [
        "# Data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    zoom_range=0.2,\n",
        "    # shear_range=15,\n",
        ")\n",
        "\n",
        "\n",
        "# datagen = ImageDataGenerator(\n",
        "#     rotation_range=25,    # Slightly increased, but not too much\n",
        "#     width_shift_range=0.2,\n",
        "#     height_shift_range=0.2,\n",
        "#     zoom_range=0.2,\n",
        "#     horizontal_flip=True,\n",
        "#     shear_range=0.1  # Just a little shear (not too aggressive)\n",
        "# )\n",
        "\n",
        "\n",
        "# datagen = ImageDataGenerator(\n",
        "#     rotation_range=20,\n",
        "#     width_shift_range=0.2,\n",
        "#     height_shift_range=0.2,\n",
        "#     horizontal_flip=True,\n",
        "#     zoom_range=0.2,\n",
        "#     shear_range=0.15,   # Skewing effect for robustness\n",
        "#     brightness_range=[0.8, 1.2],  # simulates lighting variation\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f64106e-e0b0-4620-9a11-1a26e4ba874a",
      "metadata": {
        "id": "7f64106e-e0b0-4620-9a11-1a26e4ba874a"
      },
      "outputs": [],
      "source": [
        "val_test_datagen = ImageDataGenerator()\n",
        "\n",
        "# Final datasets\n",
        "train_generator = datagen.flow(X_train, y_train, batch_size=32, shuffle=True)\n",
        "val_generator = val_test_datagen.flow(X_val, y_val, batch_size=32, shuffle=False)\n",
        "test_generator = val_test_datagen.flow(X_test, y_test, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65f8602d-9bad-407e-9579-9cc5fce8823e",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "65f8602d-9bad-407e-9579-9cc5fce8823e"
      },
      "source": [
        "# Step 4: Modelling:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df4c6d56-55ca-43e3-bd9b-165ca19f8b39",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "df4c6d56-55ca-43e3-bd9b-165ca19f8b39"
      },
      "source": [
        "### settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2be1684f-4214-4dd5-828d-a57bc3783414",
      "metadata": {
        "id": "2be1684f-4214-4dd5-828d-a57bc3783414"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, Input\n",
        "from tensorflow.keras.layers import LeakyReLU, BatchNormalization, GlobalAveragePooling2D, Dropout, Dense, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad0ff7de-3a25-4e20-b25e-4c0c66205cc0",
      "metadata": {
        "id": "ad0ff7de-3a25-4e20-b25e-4c0c66205cc0",
        "outputId": "9100ae28-7f71-4e9e-e24f-a324457767d5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.config.list_physical_devices('GPU')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abf20796-c18f-4228-a996-f04f62b78802",
      "metadata": {
        "id": "abf20796-c18f-4228-a996-f04f62b78802"
      },
      "outputs": [],
      "source": [
        "tf.test.is_gpu_available()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "988f58ac-966b-4208-9f40-a2547137b0ff",
      "metadata": {
        "id": "988f58ac-966b-4208-9f40-a2547137b0ff"
      },
      "source": [
        "#### Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83394f93-557f-4d8a-9c43-9f291f7c9cb7",
      "metadata": {
        "id": "83394f93-557f-4d8a-9c43-9f291f7c9cb7"
      },
      "outputs": [],
      "source": [
        "# Function to Evaluate Keras Model\n",
        "def evaluate_model(model, X_test, y_test, class_names):\n",
        "    # Get model predictions\n",
        "    y_pred_probs = model.predict(X_test)\n",
        "    y_pred = np.argmax(y_pred_probs, axis=1)  # Convert probabilities to class labels\n",
        "    y_true = np.argmax(y_test, axis=1)  # Convert one-hot encoded labels to class indices\n",
        "\n",
        "    # Compute accuracy\n",
        "    accuracy = accuracy_score(y_true, y_pred) * 100  # Convert to percentage\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.xlabel(\"Predicted Labels\")\n",
        "    plt.ylabel(\"True Labels\")\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.show()\n",
        "\n",
        "    # Print classification report\n",
        "    print(classification_report(y_true, y_pred, target_names=class_names))\n",
        "    print(f\"Final Accuracy: {accuracy:.2f}%\")  # Print final accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87af5a0f-34b9-4c76-baca-3f297e4bcdd3",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "87af5a0f-34b9-4c76-baca-3f297e4bcdd3"
      },
      "source": [
        "## CNN Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b31e2937-d9f7-44b6-9fa3-71b858321f41",
      "metadata": {
        "id": "b31e2937-d9f7-44b6-9fa3-71b858321f41"
      },
      "source": [
        "### Convolutional Neural Networks (CNN)\n",
        "#### 🔹 Definition\n",
        "A **Convolutional Neural Network (CNN)** is a deep learning model designed for image processing tasks. It automatically extracts features from images using convolutional layers.\n",
        "\n",
        "#### 🔹 Key Components\n",
        "- **Convolutional Layers**: Detect patterns using filters (kernels)\n",
        "- **Pooling Layers**: Reduce dimensions while preserving features\n",
        "- **Fully Connected Layers**: Classify the extracted features\n",
        "- **Activation Functions**: Typically ReLU for non-linearity\n",
        "\n",
        "#### 🔹 Advantages\n",
        "✅ Captures spatial relationships in images  \n",
        "✅ Reduces number of parameters compared to traditional networks  \n",
        "✅ Suitable for image classification, object detection, and segmentation  \n",
        "✅ no manual feature extraction & engineering needed.  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb09cac7-8d15-4376-910e-03acd38b783a",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "eb09cac7-8d15-4376-910e-03acd38b783a"
      },
      "source": [
        "## Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52442890-1009-4e1f-bf49-b7e892c69b14",
      "metadata": {
        "id": "52442890-1009-4e1f-bf49-b7e892c69b14"
      },
      "outputs": [],
      "source": [
        "# Build improved CNN model\n",
        "def build_model():\n",
        "    model = models.Sequential([\n",
        "        # Input(shape=(64, 64, 3)),\n",
        "        Input(shape=(128, 128, 3)),\n",
        "\n",
        "        layers.Conv2D(32, (3, 3), padding='same'),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(alpha=0.1),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "        layers.Conv2D(64, (3, 3), padding='same'),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(alpha=0.1),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "        layers.Conv2D(128, (3, 3), padding='same'),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(alpha=0.1),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        Dropout(0.25),\n",
        "\n",
        "        layers.Conv2D(256, (3, 3), padding='same'),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(alpha=0.1),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        Dropout(0.25),\n",
        "\n",
        "        layers.Conv2D(512, (3, 3), padding='same'),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(alpha=0.1),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        Dropout(0.25),\n",
        "\n",
        "        GlobalAveragePooling2D(),\n",
        "        Dense(256),\n",
        "        LeakyReLU(alpha=0.1),\n",
        "        Dropout(0.4),\n",
        "        Dense(len(categories), activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=0.001),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61dcebeb-57e6-4970-a4d9-49184b241b27",
      "metadata": {
        "id": "61dcebeb-57e6-4970-a4d9-49184b241b27"
      },
      "outputs": [],
      "source": [
        "# Callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d2de0b5-3f91-4f55-99cb-65ffa07f8560",
      "metadata": {
        "id": "6d2de0b5-3f91-4f55-99cb-65ffa07f8560"
      },
      "source": [
        "### 🧠 **CNN** Model Architecture – Full Explanation\n",
        "\n",
        "This document explains the architecture and logic behind the custom Convolutional Neural Network (CNN) model used to classify EuroSAT satellite images.\n",
        "\n",
        "---\n",
        "\n",
        "#### 📥 Input Layer\n",
        "```python\n",
        "Input(shape=(128, 128, 3))\n",
        "```\n",
        "- Defines the shape of the input images.\n",
        "- 128×128 pixels with 3 RGB channels.\n",
        "- Essential for compatibility with model layers.\n",
        "\n",
        "---\n",
        "\n",
        "#### 🔁 Convolutional Layers\n",
        "\n",
        "Each Layer extracts increasingly abstract spatial features.\n",
        "\n",
        "#### 🔹 Layer 1\n",
        "```python\n",
        "Conv2D(32, (3, 3), padding='same')\n",
        "BatchNormalization()\n",
        "LeakyReLU(alpha=0.1)\n",
        "MaxPooling2D((2, 2))\n",
        "```\n",
        "- 32 filters for edge detection and textures.\n",
        "- BatchNormalization stabilizes learning.\n",
        "- LeakyReLU prevents dead neurons.\n",
        "- MaxPooling reduces spatial size.\n",
        "\n",
        "#### 🔹 Layer 2\n",
        "```python\n",
        "Conv2D(64, (3, 3), padding='same')\n",
        "BatchNormalization()\n",
        "LeakyReLU(alpha=0.1)\n",
        "MaxPooling2D((2, 2))\n",
        "```\n",
        "- Deeper filters (64) capture more complex features.\n",
        "- Follows same structure as Block 1.\n",
        "\n",
        "#### 🔹 Layer 3\n",
        "```python\n",
        "Conv2D(128, (3, 3), padding='same')\n",
        "BatchNormalization()\n",
        "LeakyReLU(alpha=0.1)\n",
        "MaxPooling2D((2, 2))\n",
        "Dropout(0.25)\n",
        "```\n",
        "- Adds dropout for regularization.\n",
        "- More filters for deeper representations.\n",
        "\n",
        "#### 🔹 Layer 4\n",
        "```python\n",
        "Conv2D(256, (3, 3), padding='same')\n",
        "BatchNormalization()\n",
        "LeakyReLU(alpha=0.1)\n",
        "MaxPooling2D((2, 2))\n",
        "Dropout(0.25)\n",
        "```\n",
        "- 256 filters for high-level abstraction.\n",
        "- Continues downsampling and regularizing.\n",
        "\n",
        "#### 🔹 Layer 5\n",
        "```python\n",
        "Conv2D(512, (3, 3), padding='same')\n",
        "BatchNormalization()\n",
        "LeakyReLU(alpha=0.1)\n",
        "MaxPooling2D((2, 2))\n",
        "Dropout(0.25)\n",
        "```\n",
        "- 512 filters: top-level spatial features.\n",
        "- Final convolutional layer before pooling.\n",
        "\n",
        "---\n",
        "\n",
        "#### 🌍 Feature Aggregation\n",
        "\n",
        "```python\n",
        "GlobalAveragePooling2D()\n",
        "```\n",
        "- Averages each feature map into a single number.\n",
        "- Reduces dimensionality, avoids overfitting.\n",
        "\n",
        "---\n",
        "\n",
        "#### 🧠 Dense Layers (Classifier)\n",
        "\n",
        "```python\n",
        "Dense(256)\n",
        "LeakyReLU(alpha=0.1)\n",
        "Dropout(0.4)\n",
        "Dense(len(categories), activation='softmax')\n",
        "```\n",
        "- Dense(256): learns patterns from feature maps.\n",
        "- Dropout(0.4): prevents overfitting.\n",
        "- Final Dense uses softmax for classification across 10 EuroSAT classes.\n",
        "\n",
        "---\n",
        "\n",
        "#### ⚙️ Compilation\n",
        "\n",
        "```python\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "```\n",
        "- Adam: adaptive optimizer for faster convergence.\n",
        "- Categorical crossentropy: suitable for multi-class classification.\n",
        "- Accuracy as the evaluation metric.\n",
        "\n",
        "---\n",
        "\n",
        "#### ⏱️ Callbacks\n",
        "\n",
        "```python\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)\n",
        "```\n",
        "- EarlyStopping halts training if val_loss stagnates for 5 epochs.\n",
        "- ReduceLROnPlateau decreases learning rate for finer convergence.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4385ab84-42d7-410a-976d-babbc34366a2",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "4385ab84-42d7-410a-976d-babbc34366a2"
      },
      "source": [
        "## Model Training (Fitting)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b93f3109-9c64-4968-884b-058fc09f8031",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "b93f3109-9c64-4968-884b-058fc09f8031"
      },
      "source": [
        "### 64 resizing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "042deef5",
      "metadata": {
        "id": "042deef5"
      },
      "outputs": [],
      "source": [
        "# model building\n",
        "model_64 = build_model()\n",
        "\n",
        "# Train the model\n",
        "history = model_64.fit(\n",
        "    train_generator,\n",
        "    epochs=60,\n",
        "    validation_data=val_generator,\n",
        "    callbacks=[early_stopping, reduce_lr]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7de70ed8-0d04-4453-9191-49c2b83734dc",
      "metadata": {
        "id": "7de70ed8-0d04-4453-9191-49c2b83734dc"
      },
      "outputs": [],
      "source": [
        "evaluate_model(model_64, test_generator, y_test, class_names=categories)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "327bdf2e-ebc0-4514-84ce-447bc5030165",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "327bdf2e-ebc0-4514-84ce-447bc5030165"
      },
      "source": [
        "### 128 resizing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d127404e-1b04-4642-8a03-fc9e8d0362f4",
      "metadata": {
        "id": "d127404e-1b04-4642-8a03-fc9e8d0362f4"
      },
      "outputs": [],
      "source": [
        "# Create the model\n",
        "model_128 = build_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b5ff183-70cc-407d-ba2e-aad4248a86e3",
      "metadata": {
        "id": "9b5ff183-70cc-407d-ba2e-aad4248a86e3"
      },
      "source": [
        "#### 🏋️ Training Process\n",
        "\n",
        "```python\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=60,\n",
        "    validation_data=val_generator,\n",
        "    callbacks=[early_stopping, reduce_lr]\n",
        ")\n",
        "```\n",
        "- Trains up to 60 epochs with validation monitoring.\n",
        "- Uses generators for memory-efficient data loading.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6575c3b0",
      "metadata": {
        "id": "6575c3b0"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "history = model_128.fit(\n",
        "    train_generator,\n",
        "    epochs=60,\n",
        "    validation_data=val_generator,\n",
        "    callbacks=[early_stopping, reduce_lr]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8565313e",
      "metadata": {
        "id": "8565313e"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model on test data\n",
        "evaluate_model(model_128, test_generator, y_test, categories)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98976fa1-db79-4c12-8a90-4b3a442cbb73",
      "metadata": {
        "id": "98976fa1-db79-4c12-8a90-4b3a442cbb73"
      },
      "source": [
        "####  Final Accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e7c18e6-2584-4de3-86ab-e9fa35a4c5b4",
      "metadata": {
        "id": "4e7c18e6-2584-4de3-86ab-e9fa35a4c5b4"
      },
      "source": [
        "#### Plot Accuracy and Loss Curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c528f27-0694-4315-a373-aead8813d014",
      "metadata": {
        "id": "4c528f27-0694-4315-a373-aead8813d014"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot accuracy\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3a5b55d-7889-4637-b8fa-a884e5323ec5",
      "metadata": {
        "id": "f3a5b55d-7889-4637-b8fa-a884e5323ec5"
      },
      "source": [
        "#### Model Saving"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e329b00-f394-4294-ac2a-bfbe06f17856",
      "metadata": {
        "id": "3e329b00-f394-4294-ac2a-bfbe06f17856"
      },
      "outputs": [],
      "source": [
        "model_128.save('better_cnn_model.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "baaffe98-bb0f-4329-ad39-87faa638f548",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "baaffe98-bb0f-4329-ad39-87faa638f548"
      },
      "source": [
        "## Import and Test Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8f58d2d-17c4-41cc-aa83-1bb076faff5b",
      "metadata": {
        "id": "f8f58d2d-17c4-41cc-aa83-1bb076faff5b"
      },
      "outputs": [],
      "source": [
        "from keras.models import load_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9631661-edda-485b-85a5-17ccd26ccd1e",
      "metadata": {
        "id": "b9631661-edda-485b-85a5-17ccd26ccd1e"
      },
      "outputs": [],
      "source": [
        "model = load_model(\"best_cnn_model.h5\", compile=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "497dd5fb",
      "metadata": {
        "id": "497dd5fb"
      },
      "outputs": [],
      "source": [
        "evaluate_model(model, test_generator, y_test, categories)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "228b2152-9f1d-4e1b-96b5-e65a92d868d2",
      "metadata": {
        "id": "228b2152-9f1d-4e1b-96b5-e65a92d868d2"
      },
      "source": [
        "### Test Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "020c15b8-dd6c-4527-9ce2-58e09798ace5",
      "metadata": {
        "id": "020c15b8-dd6c-4527-9ce2-58e09798ace5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def predict_image(img_path, model, class_names):\n",
        "    # Step 1: Load image\n",
        "    original_img = image.load_img(img_path)\n",
        "    img = image.load_img(img_path, target_size=(128, 128))  # resize to match input\n",
        "\n",
        "    # Step 2: Convert to array\n",
        "    img_array = image.img_to_array(img)\n",
        "\n",
        "    # Step 3: Normalize (scale to 0-1)\n",
        "    img_array = img_array / 255.0\n",
        "\n",
        "    # Step 4: Expand dimensions (make it batch-like)\n",
        "    img_batch = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "    # Step 5: Predict\n",
        "    prediction = model.predict(img_batch)\n",
        "    class_index = np.argmax(prediction)\n",
        "\n",
        "    # Step 6: Show result\n",
        "    predicted_label = class_names[class_index]\n",
        "    print(f\"Predicted Class: {predicted_label} ({prediction[0][class_index]*100:.2f}%)\")\n",
        "\n",
        "    # Optional: Display the image\n",
        "    plt.imshow(original_img)\n",
        "    plt.title(f\"Predicted: {predicted_label}\")\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74ffe181-181a-4c74-b307-ee70ccf96415",
      "metadata": {
        "id": "74ffe181-181a-4c74-b307-ee70ccf96415"
      },
      "outputs": [],
      "source": [
        "predict_image(\"C:\", model, categories)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}